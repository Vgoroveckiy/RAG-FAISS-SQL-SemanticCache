# Документация по приложению RAG-системы для ювелирных украшений

Это консольное приложение реализует Retrieval Augmented Generation (RAG) систему для ответа на вопросы о ювелирных украшениях. Оно использует большую языковую модель (LLM) от OpenAI, векторную базу данных FAISS для хранения эмбеддингов документов и семантического кэша, а также SQLite для управления метаданными и полным текстом документов.

---

## 1. Основные компоненты и их назначение

Приложение состоит из нескольких ключевых компонентов, работающих совместно:

* **`Config`**: Класс, содержащий все пути к файлам, названия моделей и параметры для тонкой настройки системы. Все пути относительны к корневой директории приложения.
* **`DocumentStorage` (SQLite)**:
    * Отвечает за хранение **полного текстового содержимого документов** и их **метаданных** (путь к файлу, хэш, время последнего изменения).
    * Использует **хэши файлов** и **время последнего изменения** для определения, был ли файл изменён с момента последней индексации. Это позволяет избежать повторной обработки неизменившихся файлов.
    * Хранит **FAISS chunk IDs** для каждого документа. Эти ID используются для точечного удаления устаревших частей документа из векторной базы данных.
* **`VectorDatabase` (FAISS)**:
    * Управляет двумя отдельными FAISS-индексами:
        * **Основной индекс (`faiss_index`)**: Содержит векторные представления (эмбеддинги) текстовых "чанков" (сегментов) из всех проиндексированных документов. Используется для поиска релевантной информации по запросу пользователя.
        * **Семантический кэш (`cache_index`)**: Хранит пары "вопрос-ответ" в виде векторных представлений. Используется для быстрого ответа на ранее заданные или очень похожие вопросы, что снижает задержку и затраты на LLM.
    * Позволяет загружать существующие индексы или создавать новые.
    * Использует модель эмбеддингов BAAI/bge-m3 для создания векторов.
* **LangChain (LLMChain, RetrievalChain)**:
    * Интегрирует LLM (GPT-4o-mini по умолчанию) с ретривером (FAISS-индексом) для генерации ответов на основе найденного контекста.
    * Использует промпт-шаблон для формирования запроса к LLM.
* **`unstructured`**: Библиотека для извлечения текста и метаданных из различных форматов документов (PDF, DOCX, TXT).
* **Консольный интерфейс**: Предоставляет простое меню для взаимодействия с системой (индексация, чат, тесты, очистка).

---

## 2. Установка и запуск

### 2.1. Предварительные требования

* Python 3.8+
* Ключ API от OpenAI (получите на [platform.openai.com](https://platform.openai.com/))

### 2.2. Установка зависимостей

1.  Клонируйте репозиторий (если применимо) или сохраните код в файл `rag_console_app.py`.
2.  Перейдите в директорию проекта.
3.  Создайте виртуальное окружение (рекомендуется):
    ```bash
    python -m venv .venv
    ```
4.  Активируйте виртуальное окружение:
    * Для Windows:
        ```bash
        .venv\Scripts\activate
        ```
    * Для macOS/Linux:
        ```bash
        source .venv/bin/activate
        ```
5.  Установите необходимые библиотеки:
    ```bash
    pip install -q -U  langchain langchain-openai unstructured faiss-cpu pypdf "unstructured[all-docs]" python-dotenv numpy
    ```

### 2.3. Конфигурация OpenAI API Key

1.  Создайте файл с названием `.env` в корневой директории вашего проекта (рядом с `rag_console_app.py`).
2.  Добавьте в этот файл ваш ключ API от OpenAI:
    ```
    OPENAI_API_KEY="ВАШ_КЛЮЧ_OPENAI"
    ```

### 2.4. Подготовка данных

1.  Создайте папку с названием `data` в той же директории, что и `rag_console_app.py`.
2.  Поместите в эту папку ваши документы (`.pdf`, `.docx`, `.txt`) и JSON-файлы каталога (`.json`), которые вы хотите проиндексировать. Пример JSON-файла:
    ```json
    [
      {
        "name": "Кольцо с бриллиантом",
        "description": "Элегантное кольцо с крупным бриллиантом в центре.",
        "usage": "Идеально подходит для особых случаев, таких как помолвка или свадьба.",
        "price": "16000 рублей",
        "url": "https://example.com/ring_diamond"
      },
      {
        "name": "Серебряные серьги с аметистами",
        "description": "Изящные серьги из серебра с натуральными аметистами.",
        "usage": "Подходят для повседневной носки или для создания легкого вечернего образа.",
        "price": "3500 рублей",
        "url": "https://example.com/silver_earrings"
      }
    ]
    ```

### 2.5. Первый запуск и миграция базы данных

Если вы запускали предыдущие версии приложения, где не было поля `faiss_chunk_ids` в `documents.db`, вам нужно **удалить существующий файл `documents.db`** (находится в корневой директории проекта) перед первым запуском этой версии. Это позволит SQLite создать базу данных с обновленной схемой.

```bash
# Удалить файл documents.db, если он существует
rm documents.db  # Linux/macOS
del documents.db # Windows
```

### 2.6. Запуск приложения

Активируйте виртуальное окружение (если ещё не активировано) и запустите скрипт:

```bash
python rag_console_app.py
```

---

## 3. Работа с приложением (Меню)

После запуска приложения вы увидите консольное меню:

```
==================================================
          Система RAG для ювелирных украшений
==================================================
1. Очистить данные (индексы FAISS и базу данных)
2. Проиндексировать документы (из папки 'data')
3. Запустить интерактивный чат
4. Запустить тестовые вопросы
0. Выйти
==================================================
```

### 3.1. Очистить данные (Опция 1)

Эта опция выполняет **полную очистку** всех сгенерированных данных:

* Удаляет папку основного FAISS-индекса (`./faiss_index`).
* Удаляет папку семантического кэша FAISS (`./cache_index`).
* Удаляет базу данных SQLite (`./documents.db`), содержащую метаданные и полный текст документов.

Используйте эту опцию, если вы хотите начать с чистого листа или столкнулись с проблемами совместимости данных после обновления кода. **После очистки вам нужно будет заново проиндексировать документы.**

### 3.2. Проиндексировать документы (Опция 2)

Эта опция запускает процесс обработки документов из папки `data` и их индексации.

**Особенности индексации:**

* **Проверка изменений**: Перед обработкой каждого файла (и каждого элемента в JSON-каталоге) система проверяет его хэш и время последнего изменения.
    * Если файл **не изменился** с момента последней индексации, его содержимое берется из кэша SQLite, и его чанки не добавляются повторно в FAISS.
    * Если файл **изменился** (или это новый файл), происходит следующее:
        1.  **Извлечение текста**: `unstructured` парсит документ и извлекает полный текст. Для JSON-файлов каждый элемент каталога обрабатывается как отдельный "документ" с уникальным путем (например, `catalog.json#0`).
        2.  **Сохранение в SQLite**: Полный текст и метаданные документа сохраняются или обновляются в `documents.db`.
        3.  **Точечное обновление FAISS-индекса документов**: Если документ уже существовал, но изменился, старые чанки, связанные с ним, **удаляются** из FAISS-индекса документов (`faiss_index`), а затем добавляются новые чанки. Это предотвращает дублирование и устаревание данных в основном индексе.
        4.  **Точечная инвалидация семантического кэша**: **ВАЖНО!** При изменении документа система **автоматически удаляет все записи из семантического кэша (`cache_index`), которые были сгенерированы с использованием данных из этого (измененного) файла**. Это гарантирует, что при следующем запросе, затрагивающем измененные данные, кэш не вернет устаревший ответ.
* **Сегментация текста**: Извлеченный текст разбивается на более мелкие "чанки" с помощью `RecursiveCharacterTextSplitter` для более эффективного поиска.
* **Создание эмбеддингов**: Для каждого чанка создается векторное представление (эмбеддинг) с помощью модели `BAAI/bge-m3`.
* **Добавление в FAISS**: Эмбеддинги чанков добавляются в основной FAISS-индекс.

### 3.3. Запустить интерактивный чат (Опция 3)

Переводит приложение в режим диалога с пользователем.

**Особенности работы в чате:**

* **Проверка кэша**: При каждом запросе система сначала проверяет **семантический кэш (`cache_index`)**.
    * Если найден достаточно похожий вопрос (по умолчанию порог сходства `L2 <= 0.1`), возвращается закэшированный ответ, что значительно ускоряет работу и экономит токены LLM.
    * Если кэшированного ответа нет или он недостаточно похож:
        1.  **Поиск релевантных документов**: Система использует запрос пользователя для поиска наиболее релевантных текстовых чанков в основном FAISS-индексе (`faiss_index`).
        2.  **Генерация ответа LLM**: Найденные релевантные чанки (контекст) передаются LLM вместе с запросом пользователя для генерации ответа.
        3.  **Кэширование нового ответа**: Сгенерированный LLM ответ кэшируется в `cache_index` вместе с исходным вопросом и списком файлов-источников, которые были использованы для генерации ответа.
* **Отображение контекста**: В консоль выводится информация о релевантных документах, найденных ретривером (их источник, название элемента, часть содержимого).
* **Выход**: Для завершения чата введите `выход` или `exit`.

### 3.4. Запустить тестовые вопросы (Опция 4)

Выполняет серию предопределенных вопросов для демонстрации функционала и проверки ответов системы. Это удобно для быстрой проверки после индексации или изменения данных.

### 3.5. Выйти (Опция 0)

Завершает работу приложения, закрывая все соединения с базами данных.

---

## 4. Особенности работы с кэшем

### 4.1. Семантический кэш (`cache_index`)

* **Назначение**: Ускорение ответов и экономия токенов LLM.
* **Принцип работы**: Хранит векторные представления вопросов и соответствующие им ответы. При новом запросе, система ищет похожий вопрос в кэше.
* **Порог сходства (`similarity_threshold`)**: Определяет, насколько близко должен быть новый вопрос к закэшированному, чтобы кэшированный ответ был использован. Для L2 расстояния, чем меньше значение, тем "ближе" должны быть векторы. По умолчанию 0.1.
* **Кэширование ответов**: Ответы, сгенерированные LLM (как с использованием ретривала, так и без него), автоматически добавляются в кэш.
* **`sources` в кэше**: Каждая запись в кэше теперь содержит поле `sources` в метаданных. Это список **базовых имен файлов** (например, `catalog.json`, `my_document.pdf`), которые были использованы в контексте при генерации данного ответа. Это критически важно для точечной инвалидации.

### 4.2. Точечная инвалидация кэша

Это ключевая особенность для поддержания актуальности ответов при изменении данных без полной очистки кэша.

* **Как это работает**:
    1.  Когда вы запускаете **Опцию 2 ("Проиндексировать документы")**, и система обнаруживает, что файл в папке `data` (или элемент в JSON-каталоге) **изменился**:
    2.  Система извлекает **базовое имя файла** (например, если обновился элемент `data/catalog.json#0`, базовое имя будет `catalog.json`).
    3.  Затем она вызывает метод `delete_cached_entries_by_source(source_file_name)` в `VectorDatabase`.
    4.  Этот метод просматривает все записи в `cache_index`.
    5.  **Если запись в кэше была создана на основе контекста, включающего измененный файл (то есть `source_file_name` присутствует в её метаданных `sources`), эта запись удаляется из кэша.**
    6.  После удаления устаревших записей, измененный файл заново индексируется в основной FAISS-индекс.
* **Преимущества**:
    * Сохраняет актуальные ответы для вопросов, которые не зависят от измененных данных.
    * Экономит время и ресурсы, избегая полной перегенерации кэша.
* **Ограничения**:
    * Если один файл (например, большой `catalog.json`) содержит множество несвязанных элементов, изменение одного элемента приведет к инвалидации всех кэшированных ответов, которые использовали *любой* элемент из этого файла. Это компромисс, так как отслеживать влияние каждого мельчайшего изменения на каждый кэшированный ответ крайне сложно без значительного усложнения архитектуры.

---

## 5. Дополнительные замечания

* **Конфигурация**: Вы можете изменять параметры в классе `Config` (`EMBEDDING_MODEL`, `LLM_MODEL`, `LLM_TEMPERATURE`, `chunk_size`, `chunk_overlap` и др.) для оптимизации работы системы под ваши нужды.
* **FAISS-индекс**: FAISS-индексы сохраняются на диск, что позволяет не пересоздавать их каждый раз при запуске приложения. Однако, при значительном изменении моделей эмбеддингов или структуры данных, рекомендуется очистить и пересоздать индексы.
* **Масштабируемость**: Текущая реализация подходит для умеренных объемов данных. Для очень больших объемов данных (десятки и сотни тысяч документов) могут потребоваться более производительные векторные базы данных (например, Weaviate, Pinecone, Qdrant) и распределённые системы обработки.

---

