# Система RAG для ювелирных украшений

Эта система представляет собой реализацию **Retrieval Augmented Generation (RAG)**, разработанную для ответа на вопросы о ювелирных украшениях. Она использует комбинацию **локального хранилища документов (SQLite)**, **векторной базы данных (FAISS)** для хранения эмбеддингов документов и кэша вопросов/ответов, а также **большой языковой модели (LLM) LangChain** для генерации ответов.

---

## Возможности

* **Индексирование документов**: Автоматически парсит и индексирует документы в форматах PDF, DOCX и TXT из заданной директории.
* **Инкрементальная обработка**: Отслеживает изменения в файлах с помощью хеширования и меток времени, обрабатывая только новые или изменённые документы.
* **Векторный поиск**: Использует FAISS для эффективного поиска наиболее релевантных фрагментов документов на основе семантической близости к запросу пользователя.
* **Интеллектуальное кэширование**: Кэширует ответы на ранее заданные вопросы, чтобы быстро предоставлять ответы на повторяющиеся или похожие запросы, снижая нагрузку на LLM и ускоряя процесс.
* **Генерация ответов LLM**: Использует GPT-4o-mini для генерации подробных и контекстуально релевантных ответов, основанных на извлечённой информации.
* **Кастомизированный промпт**: Включает специализированный промпт для LLM, ориентированный на ответы по ювелирным украшениям, их характеристикам и ценам, с рекомендациями по выбору.

---

## Архитектура и компоненты

### 1. Конфигурация (`Config`)

Класс `Config` централизует все настраиваемые параметры системы:

* `INPUT_DIR`: Директория для исходных документов.
* `FAISS_INDEX_PATH`: Путь для сохранения основного FAISS индекса.
* `SQL_DB_PATH`: Путь к файлу базы данных SQLite для метаданных документов.
* `CACHE_INDEX_PATH`: Путь для сохранения FAISS индекса кэша вопросов/ответов.
* `EMBEDDING_MODEL`: Название модели эмбеддингов (по умолчанию "BAAI/bge-m3").
* `LLM_MODEL`: Название модели LLM (по умолчанию "gpt-4o-mini").
* `LLM_TEMPERATURE`: Температура LLM для контроля креативности ответов.
* `SUPPORTED_EXTENSIONS`: Кортеж поддерживаемых расширений файлов.

### 2. Инициализация LLM и эмбеддингов

* **`load_dotenv()`**: Загружает переменные окружения из файла `.env` (например, `OPENAI_API_KEY`).
* **`HuggingFaceEmbeddings`**: Создает объект для генерации векторных представлений текста.
* **`ChatOpenAI`**: Инициализирует языковую модель от OpenAI.

### 3. Хранилище документов (`DocumentStorage`)

* Использует **SQLite** для персистентного хранения метаданных и содержимого обработанных документов.
* Сохраняет `file_path`, `file_hash`, `last_modified`, `content` и `metadata` каждого документа.
* Позволяет быстро проверять, был ли файл уже обработан или изменён.

### 4. Векторная база данных (`VectorDatabase`)

* Основана на **FAISS** для эффективного поиска по подобию.
* **Основной индекс (`self.db`)**: Хранит эмбеддинги всех обработанных документов.
* **Индекс кэша (`self.cache_db`)**: Хранит эмбеддинги вопросов и соответствующие им ответы.
* Реализует логику загрузки, создания, добавления и поиска в обоих индексах.

### 5. Функции обработки файлов

* **`get_file_hash(file_path)`**: Генерирует SHA256-хэш файла для отслеживания изменений.
* **`parse_files(directory, doc_storage)`**:
    * Сканирует указанную директорию.
    * Проверяет `DocumentStorage` для инкрементальной обработки.
    * Использует `unstructured.partition.auto.partition` для извлечения текста из различных форматов документов.
    * Преобразует извлечённый текст в объекты `langchain_core.documents.Document`.

### 6. Система RAG (`RAGSystem`)

* **Оркестратор**: Собирает все компоненты (хранилище документов, векторную базу, LLM) воедино.
* **Инициализация**: При запуске парсит документы, строит/загружает векторные индексы и настраивает цепочки LangChain.
* **Промпт**: Определяет пользовательский промпт для LLM, включающий контекст из документов и вопрос пользователя.
* **Цепочка `RetrievalQA`**: Основной компонент LangChain, который выполняет:
    1.  Получение релевантного контекста с помощью `retriever`.
    2.  Передачу контекста и вопроса в LLM для генерации ответа.
* **Метод `query(question, use_cache=True)`**:
    * Сначала ищет ответ в кэше `VectorDatabase`.
    * Если не найдено, выполняет запрос через `RetrievalQA` цепочку.
    * Сохраняет сгенерированный ответ в кэш.

---

## Установка и запуск

### Требования

* Python 3.8+
* Ключ API OpenAI (`OPENAI_API_KEY`)

### Установка зависимостей

```bash
pip install -r requirements.txt